{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the IRIS dataset\n",
    "iris = load_iris()\n",
    "X = iris.data.copy()\n",
    "y = np.array([iris.target_names[yi] for yi in iris.target])\n",
    "n_quantiles = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simple quantization, maps each value to which quantile it belongs\n",
    "def quantize(x, n_intervals=3):\n",
    "    p = np.percentile(x, np.linspace(0, 100, num=(n_intervals+1)))\n",
    "    return np.array([max(sum(xi > p), 1) for xi in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Discretize features using simple quantile encoding, since we are\n",
    "# implementing a multinomial version of the naive Bayes\n",
    "for feature in range(X.shape[1]):\n",
    "    X[:, feature] = quantize(X[:, feature], n_quantiles)\n",
    "X = X.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Num. train instances:', 100)\n",
      "('Num. test instances:', 50)\n",
      "('Num. features:', 4)\n",
      "[2 2 2 2]\n",
      "[2 1 2 2]\n",
      "versicolor\n",
      "(100,)\n",
      "versicolor\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "print(\"Num. train instances:\", len(X_train))\n",
    "print(\"Num. test instances:\", len(X_test))\n",
    "print(\"Num. features:\", X.shape[1])\n",
    "print(X_train[0])\n",
    "print(X_test[0])\n",
    "print(y_train[0])\n",
    "print(y_train.shape)\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Num. insances, num. features and num. classes\n",
    "classes = np.unique(y)\n",
    "n, d, C = X_train.shape[0], X_train.shape[1], len(classes)\n",
    "\n",
    "# Laplace smoothing factor\n",
    "alpha = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We store the class probabilites in this dictionary\n",
    "class_probs = {}\n",
    "\n",
    "# We compute each probability according to the formula above\n",
    "for c in classes:\n",
    "    class_probs[c] = (np.array(y_train == c).sum() + alpha) / (n + C*alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[set([1, 2, 3]), set([1, 2, 3]), set([1, 2, 3]), set([1, 2, 3])]\n"
     ]
    }
   ],
   "source": [
    "# First, we get the list of possible values for each feature:\n",
    "possible_values = [set(X_train[:, feature]) for feature in range(d)]\n",
    "print(possible_values)\n",
    "# We will store these probabilities in this dictionary\n",
    "# The key of the dictionary is a pair of (feature, class) such that:\n",
    "#   feature_probs[j, c]  \n",
    "# gives a dictionary which maps each possible value of the j-th feature to \n",
    "# its probability, given each class, as we saw in the formula above, i.e.\n",
    "# P(X_{i,j} = x | y_i = c) can be obtained from: feature_probs[j, c][k]\n",
    "feature_probs = {(j, c): {v: 0 for v in possible_values[j]} \n",
    "                 for c in classes for j in range(d)}\n",
    "\n",
    "# Now, we compute the above probabilites, for each feature, given each class\n",
    "for j in range(d):\n",
    "    for c in classes:\n",
    "        # This gives us the j-th feature of instances in class c\n",
    "        in_class_c = X_train[y_train == c, j]\n",
    "        for x in possible_values[j]:\n",
    "            numerator = sum(in_class_c == x) + alpha\n",
    "            denominator = len(in_class_c) + len(possible_values[j])*alpha\n",
    "            feature_probs[j, c][x] = numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Baldwin (EAPS) Python 2",
   "language": "python",
   "name": "mebaldwi-eaps-python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
