{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pylab\n",
    "\n",
    "X, _ = make_moons(n_samples=5000, random_state=42, noise=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pylab.scatter(X[:,0], X[:,1])\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the torch functions\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the z (the random input to the GAN)\n",
    "def z(n):\n",
    "    return torch.Tensor(np.random.normal(0, 1, (n,2)))  # Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generator\n",
    "g_input_size = 2     # Dimension of random noise given to generator\n",
    "g_hidden_size = 50   # Number of hidden units of the generator complexity\n",
    "g_output_size = 2    # size of generated output vector\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.layer3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.layer1(x)) # ELU activations are like ReLU activations but without zero gradients for ELU(x), x < 0.\n",
    "        x = F.sigmoid(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generator creation\n",
    "G = Generator(input_size=g_input_size, hidden_size=g_hidden_size, output_size=g_output_size)\n",
    "\n",
    "g_learning_rate = 2e-4\n",
    "optim_betas = (0.9, 0.999)\n",
    "\n",
    "g_optimizer = optim.Adam(G.parameters(), lr=g_learning_rate, betas=optim_betas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Discriminator (classifier) creation\n",
    "# Define the mini-batch sizes\n",
    "minibatch_size = 100\n",
    "\n",
    "d_input_size = 2     # Input to the discriminator (will determine 'real' or 'fake')\n",
    "d_hidden_size = 50   # Number of hidden units of the discriminator\n",
    "d_output_size = 1    # Output of discriminator for 'real' vs. 'fake' classes\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.layer3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.layer1(x))\n",
    "        x = F.elu(self.layer2(x))\n",
    "        return F.sigmoid(self.layer3(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D = Discriminator(input_size=d_input_size, hidden_size=d_hidden_size, output_size=d_output_size)\n",
    "\n",
    "# Negative log-likelihood loss (aka, cross-entropy loss) for binary classes (fake, real)\n",
    "loss = nn.BCELoss() \n",
    "\n",
    "d_learning_rate = 2e-4\n",
    "optim_betas = (0.9, 0.999) # paramters specific to Adam optimnization (not part of this lecture)\n",
    "\n",
    "d_optimizer = optim.Adam(D.parameters(), lr=d_learning_rate, betas=optim_betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "D.zero_grad()\n",
    "\n",
    "#  Train D on real data\n",
    "d_real_data = Variable(torch.FloatTensor(X[0:minibatch_size,:]))\n",
    "d_real_decision = D(d_real_data)\n",
    "\n",
    "print(d_real_decision[0:3,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_real_error = loss(d_real_decision, Variable(torch.Tensor([[1]]*100)))  # ones = true data\n",
    "d_real_error.backward() # store gradients, but don't change params yet because we are not done with the examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d_gen_input = Variable(z(minibatch_size)) # Input to the Generator\n",
    "d_fake_data = G(d_gen_input).detach()  # Generate outputs and detach to avoid training the Generator on these labels... \n",
    "                                       # ...just get the examples\n",
    "d_fake_decision = D(d_fake_data)  # Perform a foward pass using the fake data\n",
    "print(d_fake_decision[0:3,:]) # print the decitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_fake_error = loss(d_fake_decision, Variable(torch.Tensor([[0]]*100)))  # zeros = fake\n",
    "d_fake_error.backward() # this will append the gradients of these examples to the \n",
    "                          # previous gradients (computed over the real data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_optimizer.step()     # Only optimizes Discriminator's parameters; \n",
    "                      #   the updates are based on the stored gradients from backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G.zero_grad() # must zero the Generator gradient\n",
    "\n",
    "d_gen_input = Variable(z(minibatch_size))\n",
    "d_fake_data = G(d_gen_input).detach()  # detach to avoid training G on these labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dg_fake_decision = D(d_fake_data)\n",
    "\n",
    "# we want to fool the classifier, so pretend it's all real data and we will train G to \n",
    "#   minimize this loss (maximize the likelihood of getting the fake data to have labels \"1\" (true data))\n",
    "g_error = loss(dg_fake_decision, Variable(torch.Tensor([[1]]*100)))  \n",
    "\n",
    "g_error.backward() # Get Generator gradients\n",
    "\n",
    "g_optimizer.step()  # Only optimizes G's parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "\n",
    "d_steps = 20 # number of gradient steps of discriminator per minibatch\n",
    "g_steps = 1 # number of gradient steps of generator per minibatch\n",
    "\n",
    "print_interval = 10\n",
    "batch_no = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for d_index in range(d_steps):\n",
    "        # 1. Train Discriminator on real+fake\n",
    "\n",
    "        D.zero_grad()\n",
    "\n",
    "        #  Train D on real data\n",
    "        d_real_data = Variable(torch.FloatTensor(X[(batch_no*minibatch_size):((batch_no+1)*minibatch_size),:]))\n",
    "        d_real_decision = D(d_real_data)\n",
    "        d_real_error = loss(d_real_decision, Variable(torch.Tensor([[1]]*100)))  # ones = true data\n",
    "        d_real_error.backward() # store gradients, but don't change params yet\n",
    "        \n",
    "        batch_no += 1\n",
    "        if batch_no >= 5000/minibatch_size:\n",
    "            batch_no = 0\n",
    "        \n",
    "        d_gen_input = Variable(z(minibatch_size))\n",
    "        d_fake_data = G(d_gen_input).detach()  # Generate outputs and detach to avoid training the Generator on these labels\n",
    "        d_fake_decision = D(d_fake_data)\n",
    "        d_fake_error = loss(d_fake_decision, Variable(torch.Tensor([[0]]*100)))  # zeros = fake data\n",
    "        d_fake_error.backward()\n",
    "        d_optimizer.step()     # Only optimizes D based on stored gradients from both backwards\n",
    "\n",
    "    for g_index in range(g_steps):\n",
    "        # 2. Train Generator on Discriminator's response (but WE WILL NOT train Discriminator on these labels)\n",
    "        G.zero_grad()\n",
    "\n",
    "        d_gen_input = Variable(z(minibatch_size))\n",
    "        d_fake_data = G(d_gen_input)  # we will be training G on these labels\n",
    "        dg_fake_decision = D(d_fake_data)\n",
    "        g_error = loss(dg_fake_decision, Variable(torch.Tensor([[1]]*100)))  # we want to fool, so pretend it's all genuine\n",
    "        g_error.backward()\n",
    "        g_optimizer.step()  # Only optimizes G's parameters\n",
    "\n",
    "    if epoch % print_interval == 0:\n",
    "        print(\"%s\" % (epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "noise = z(5000).numpy()\n",
    "\n",
    "pylab.scatter(noise[:,0], noise[:,1])\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d_gen_input = Variable(z(5000))\n",
    "d_fake_data = G(d_gen_input).detach().data.numpy()\n",
    "\n",
    "pylab.scatter(X[:,0], X[:,1],c=\"blue\")\n",
    "pylab.scatter(d_fake_data[:,0], d_fake_data[:,1],c=\"red\")\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
